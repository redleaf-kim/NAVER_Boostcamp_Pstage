## 📚 문장 내 개체간 관계 추출 Relation Extraction

###### 📌 본 프로젝트는 [_*Naver AI Boostcamp*_](https://www.edwith.org/bcaitech1/)에서 Team Project로 진행됐습니다.
<br>

## 목차
+ [최정결과](#Result)
+ [폴더구조](#Directory)
+ [소스코드설명](#Code)
+ [기술적시도](#기술적-시도들)
    + [모델](#모델)
    + [불균형](#불균형)
    + [데이터](#데이터)
+ [어려웠던 점 & 반성할점](#어려웠던-점-반성할-점)
+ [좋았던 점 & 배운 점](#좋았던-점-배운-점)


----
### 🍀 최종 결과 <a name = 'Result'></a>
- [[Relation Extraction]](http://boostcamp.stages.ai/competitions/4/overview/description)
    - **42등 (총 135팀)**
    - public  LB : 80.1%
    - private LB : 80.1%
<br></br>

### 🗄 폴더 구조 <a name = 'Directory'></a>
```
└── Relation_Extraction
     ├── aug_data
     ├── data
     └── src
          ├── experiments
          ├── enriching_inference_ensemble.py
          ├── enriching_inference.py
          ├── enriching_kfold.py
          ├── enriching_train.py
          ├── load_data.py
          ├── loss.py     
          └── enriching_model.py
```
<br></br>

### 💻 소스 코드 설명 <a name = 'Code'></a>
- `enriching_inference_ensemble.py` : Ensemble enriching model 추론코드
- `enriching_inference.py` : 단일 enriching model 추론코드
- `enriching_kfold.py` : Enriching model kfold 학습코드
- `enriching_model.py` : [[Paper]](https://arxiv.org/pdf/1905.08284.pdf)에 소개한 Enriching model 코드
- `enriching_model.py` : 단일 Enriching model 학습코드
- `load_data.py` : 데이터셋 로딩코드
- `loss.py` : Label Smoothing
<br></br>
---
## 기술적 시도들

- [x]  시도해 봤지만 잘 동작하지 않은 것들
- [ ]  시도하고 좋아서 최종 모델에 사용한 것들

### [**모델**]

- `상황`: 문장 내에서 Entity간의 관계를 정의하는것이 목표 → 베이스라인에서는 기존의 문장 앞에 Entity 토큰을 더해주는 방법을 사용
- `가설`: `인코딩된 문장 정보에서 Entity에 해당하는 부분을 추출하여 모델에 추가로 제공해준다면 더 좋은 성능이 나올 것임` → 해당 가설에 맞는 논문을 발견했고, 시도해봤음
    - [[참고논문]](https://arxiv.org/pdf/1905.08284.pdf) → BERT 혹은 다른 Pre-trained Language Model을 활용하여, 객체의 앞뒤로 `#`, `$`를 추가하여 추후에 분류 시에 해당 부분의 정보를 추출하여 더욱 많은 정보를 통해 분류를 수행하여 성능을 끌어 올리는 방법
- `결과`: 베이스라인으로 주어진 모델과 같은 조건하에 비교했을 때, **조금 더 높은 성능을 보임** → 계속해서 해당 모델을 사용하였음

### [**불균형**]
- [x]  Class Weight
    - `상황`: 데이터가 매우 불균형했음 → 극단적으로 1개의 데이터만 있는 클래스가 존재
    - `가설`: 학습 시에 해당 데이터에 가중치를 부여함으로써 라벨마다 다른 가중치를 갖고 학습하도록 유도한다면 Pstage-1에서 효과를 봤던 것처럼 효과를 볼 수 있을 것임
    - `결과`: Public LB score가 동일 조건하에 하락하는 현상을 보임
    - `정리`: Public LB Score가 높게 나온 prediction 결과를 살펴보면, 관계없음인 0라벨이 굉장히 많이 차지함 → Class weight을 통해서 학습을 해주게 되어 라벨을 기존보다 고르게 예측하게 됨으로서 발생한 현상으로 보임 → 테스트 셋 자체도 Imbalance한 상황이라서 오히려 점수가 떨어진 것으로 판단됨

- [ ]  Label Smoothing
    - `상황`: 데이터가 매우 불균형 → 잘못 정의된 라벨도 존재
    - `가설`: 모델이 데이터의 과반수를 차지하는 라벨에 overconfident해지는 현상이 생길 것임 → label smoothing을 적용하면 모델이 더 일반화가 되어 Performance가 좋아질 것임
    - `결과`: Public LB 대비 약 2점정도 점수가 향상
    - `정리`:
        - 분류 문제에서 label 이 3이라고 한다면, 모델에게 해당 sample을 100%로 label이 3인 상황으로 확신하게 훈련하는 것임 → label을 정수로 부여하는 것이 아니라, 다른 label이 될 수도 있는 가능성을 추가해 줌으로 overconfident해지는 경향을 막고 일반화에 도움을 줌
        - Pstage-1의 경우에는 잘 동작하지 않았었는데, 이번 스테이지에서는 잘 동작한 것으로 봐서, 어떤 경우에나 잘 동작하는 것은 아니고 잘 먹히는 경우와 그렇지 않은 경우가 존재한다는 것을 배웠음 → 어떤 경우에 잘 동작하고 그렇지 않은지 좀더 탐구해볼 필요가 있을 것 같음

### [**데이터**]
- [x]  Back Translation
    - `상황`: 총 9000개의 데이터중 특정 라벨의 데이터가 20개 미만으로 존재
    - `가설`: back translation을 활용해서 해당 라벨의 데이터를 추가해주면 모델에 다양성을 더해주어 좋은 성능이 나올 것임
    - `결과`: 오히려 Public LB 기준으로 성능이 낮아짐
    - `정리`:
        - 테스트 데이터 기준으로만 보았기 때문에 성능이 낮아진 것이 아닐까 생각도 해보지만, Synthetic Source를 만들어내는 번역과정에서 객체간의 관계를 추론하는데 도움이 되는 유용한 정보를 가진 핵심 단어들이 원문의 의미와 다르게 번역된다면 오히려 성능이 낮아질 수 있다고 생각함.
        - 번역 태스크에서는 모델에 다양성을 추가해주어 번역의 유창성을 높여줄 수 있지만 객체 분류 문제에서는 객체간의 관계를 정의할 수 있는 핵심 단어들의 의미가 퇴색되지 않는 선에서의 translation이 필요한게 아닐지? 그렇게 하기 위해서는 어떻게 해야할지 고민하게 되었음


- [ ]  EDA(Easy Data Augmentation)
    - `상황`: 총 9000개의 데이터중 특정 라벨의 데이터가 20개 미만으로 존재
    - `가설`: EDA를 통해서 RI(랜덤 삽입)는 본래와 같이 적용, SR(유의어로 교체)와 RD(랜덤 삭제)는 객체를 제외한 나머지 단어에 적용한다면 객체간의 관계를 오염하지 않고 문장의 다양성을 보장한채로 성능을 올릴 수 있을 것임
    - `결과`: 조금의 성능 향상이 존재
    - `정리`:
        - Back Translation과 달리 객체간의 관계를 보존하면서 문장의 다양성을 확보했기 때문에 성능 향상으로 이어진 것이라고 생각함
        - **처음에는 SR, RD, RI를 상황별로 다르게 적용하지 않고 모두 일괄적용 했었는데 성능이 떨어짐 → SR과 RD에서 객체 자체를 삭제하거나 객체의 단어를 바꿔버리는 현상이 생겨서 그런 것으로 생각하고 해당 부분만 다르게 적용해주니 성능이 향상되었음**
        - ~~베이스라인 모델에만 적용해보고 실제로 제출한 모델에는 적용을 못해봤음, 객체의 pos 위치를 같이 넘겨줘야하는데 귀찮음이 모든 것을 이겨버린 상황... → 반성하자 ㅜㅜ~~


- [ ]  Max Length
    - `상황`: 문장별로 인코딩 했을때 길이가 달라짐
    - `가설`: RoBERTa 모델을 기반으로한 Enriching 모델을 사용하고 있음 → 객체 토큰을 따로 베이스라인과 같이 앞에 추가해주는 것이 아니라 RoBERTa에 문장을 얹어서 나온 결과에서 객체에 해당하는 부분을 추출하는 상황 → **토크화 결과의 길이 max length를 늘려주면 성능이 올라갈 것임**
    - `결과`: max length가 무작정 늘어난다고 성능이 향상되지 않음 → 100부터 50단위로 늘려줬을때 200까지는 계속해서 증가했지만 250부터는 감소하는 현상발생
    - `정리`:
        - 아무래도 문장 내에서 객체의 정보를 추가로 추출하는 상황에서 max length에 의해서 객체의 정보 자체가 짤리는 상황이 많이 발생 → max length를 늘려주면 성능이 올라감
        - 하지만 무작정 max length 를 늘린다고 성능이 올라가지는 않음 → 95%이상의 데이터의 인코딩 길이가 190이하인 것을 볼때 200까지는 성능향상이 보장되지만 이후 부터는 성능이 하락하는 이유가 설명이 됨
            - [[참고| 토론글]](http://boostcamp.stages.ai/competitions/4/discussion/post/120)


## [**어려웠던 점 & 반성할 점**]
- 데이터 불균형
    - 총 데이터 9000개에서 일부 데이터는 10개 혹은 20개미만의 샘플을 가지고 있어서 해당 부분을 어떻게 컨드롤 해줘야할지 매우 고민이 되었음
    - 모델 학습시에 20%정도를 Validation으로 활용하면 Validation 스코어와 Public score가 유사하게 나온것으로 봐서 훈련데이터와 테스트데이터의 분포가 매우 유사하다고 생각했음
    - 그렇다면 `Public score가 높게 나온다면 모델의 일반화가 잘 된것인가?` 라는 의문이 생김
        - 훈련 데이터의 경우에는 분포가 매우 극단적으로 불균형했음 → 관계없음 데이터는 4000여개정도로 데이터의 절반에 가까운 양인데 그렇다면 테스트 데이터로 나온 성능이 높다고하여도 해당 모델에는 일반성이 확보 되었다고 할 수 없을 텐데 이대로 괜찮은 것인가에 대한 의문은 아직 있음
        - Pstage-1 랩업 리포트에 대해 다음과 유사한 느낌의 피드백을 받았음

            > 학습 데이터의 분포와 비슷한 데이터들 내에서 테스트 데이터들도 구성이 되는고 현실에서의 문제도 별반 다르지 않을 것이라고 가정합니다.

            **그렇다면 실제로 서비스에 모델을 적용할 때도, 위와 같은 가정을 가지고 서비스를 개발하는 것인가?** 서비스에 적용하는 모델은 소비자에게 문제가 될 수도 있는 민감한 결과를 추론할 수 있을 텐데 막연하게 `그렇다라는 가정하에 서비스를 제공하는 것이 과연 맞는 것인지?` 아니면 그에 대응하기   위한 다른 기법들이 있는지 궁금해졌음 

            → 사실 해결하기 굉장히 어려운 문제라고 생각되고 


- 아쉬운 공유문화 참여도
    - 피어세션에서의 공유는 활발하게 진행했지만, 토론 글을 통해서 공유하려고 하는 노력이 아직도 부족하다고 느낌
    - 면접 준비나 코딩테스트 준비를 함께 진행한다고 바빴다는 이유로 지난번의 다짐이 무의미하게도 공유에 대한 적극성이 부족했음.. → 단번에 개선되리라고는 생각하지 않았지만 결심했던 것에 반해 너무 초라했음 → 아직 두번의 기회가 더 있으니 노력해보자!


- 토크나이즈과정
    - 토큰나이즈를 할때 #, $를 객체의 양쪽에 넣어주기 위해서 직접 구현을 했음 → 패딩과 절단을 할때, Special token을 고려하지 않고 잘라내거나 패딩을 해주었던 것을 너무 늦게 알아서 좀 아쉬웠음
    - 또한 BERT와 RoBERTa의 토크나이저가 다른 special token을 사용하는지 모르고 모델의 전환이 있었을 때, [CLS], [SEP]토큰을 그대로 RoBERTa모델에 적용했던 것도 너무 늦게 알아서 아쉬웠음
    - 아쉬운점이긴 하지만 처음에는 이해하지 못했던 Special token의 사용이유와 토크나이즈 과정에 대해서도 깊에 이해할 수 있어서 아주 유익한 경험이었음 → ~~대회 자체에서 토크나이즈 과정까지 함께 경험할 수 있게 해줬다면 좀 더 유익하지 않았을까? 생각도 했음~~


### [좋았던 점, 배운 점]
- NLP에 대해서 공부할 수 있는 소중한 시간
    - 비전에 더 흥미가 많다보니 자꾸 NLP에 대한 공부의 우선순위를 미루어 왔었고 `기아현상`으로 이어졌음
    - 부스트캠프를 통해서 우선순위를 강제로 조정해줌에 따라 자꾸 미루던 NLP쪽의 여러 연구들을 접하고 실제로 대회형 프로젝트를 통해서 활용해봄으로써, 큰 줄기에서는 비전문제와 크게 다르지 않지만 데이터를 바라보는 관점을 달리해야 한다고 많이 느꼈음
        - 결국 태스크에 따라서 데이터를 다루는 방식이나 모델이 집중해야할 영역을 조절해보는 것과 같이 데이터를 기반으로 접근해야 좋은 결과가 나온다는 것을 많이 느꼈음
        - **익숙함에 갇혀 NLP라는 재미있는 영역을 자꾸 비전식으로 접근하여, 왜 배치 정규화가 아니라 Layer 정규화를 사용하는지에 대한 고민을 계속하였음**
            - 개인적으로 내린 결론이지만 정리하자면, NLP영역에서 문장의 길이는 고정된 것이 아니기 때문이라는 것임 → 같은 길이로 전처리해서 모델이 얹어준다면, 결국 padding영역이 생기고 이를 배치 정규화에서 다룰때 어떤 mean, std값이 좋을지 결정하는 것에 있어 어려움을 겪게 하는 것이라고 생각됨
            - 따라서 feature 차원에서 정규화를 해주는 Layer정규화가 더 잘 동작하는 것으로 보임
            - **→ 이렇게 task specific한 데이터의 특징을 고려하지 않고 무작정 좋은데 왜 사용하지 않지? 비전영역의 익숙함에 속아 편협한 시각을 나도 가지고 있다는 것을 깨달을 수 있어서 너무 좋았음**


- 오피스아워
    - 이전까지 대회를 하면서 무작정 `좋다던데? → 써보자! → 별로네? → 안써야지` 의 멍청한 흐름을 가지고 좋다는 것은 다 때려넣어서 돌려보고 있었음
    - 프로젝트를 할때나 대회를 할때 `문제정의 → 가설 → 실험 → 결과`의 흐름을 사용하는 것이 확실히 바람직한 방향이라는 것을 배울 수 있었음
